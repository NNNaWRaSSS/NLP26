{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BU3kSaoPruY"
      },
      "source": [
        "# N-gram Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7MIKySXmPruZ"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4_WGIq2Prua",
        "outputId": "9318df5b-019d-43d6-94d0-2a7bb1ba1d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CORPUS EXAMPLE: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'the', 'jury', 'further', 'said', 'in']\n",
            "\n",
            "\n",
            "VOCAB EXAMPLE: ['prudential', 'ah-ah', 'conant', 'fox-terrier', 'hp.', 'speedily', 'propagandists', 'fishery', 'finger-post', \"nick's\"]\n"
          ]
        }
      ],
      "source": [
        "# Loading the corpus\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "corpus = brown.words()\n",
        "\n",
        "# Case folding and getting vocab\n",
        "lower_case_corpus = [w.lower() for w in corpus]\n",
        "vocab = set(lower_case_corpus)\n",
        "\n",
        "print('CORPUS EXAMPLE: ' + str(lower_case_corpus[:30]) + '\\n\\n')\n",
        "print('VOCAB EXAMPLE: ' + str(list(vocab)[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHQZm_eTPrub",
        "outputId": "3c708e59-05fd-40c2-e029-19c0c9f31dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in Corpus: 1161192\n",
            "Vocab of the Corpus: 49815\n"
          ]
        }
      ],
      "source": [
        "print('Total words in Corpus: ' + str(len(lower_case_corpus)))\n",
        "print('Vocab of the Corpus: ' + str(len(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yr8vrFMPrub",
        "outputId": "0f7c99f1-2049-4616-f05f-6274a3a0be9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example, count for bigram ('the', 'king') is: 51\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = {}\n",
        "trigram_counts = {}\n",
        "\n",
        "# Sliding through corpus to get bigram and trigram counts\n",
        "for i in range(len(lower_case_corpus) - 2):\n",
        "    # Getting bigram and trigram at each slide\n",
        "    bigram = (lower_case_corpus[i], lower_case_corpus[i+1])\n",
        "    trigram = (lower_case_corpus[i], lower_case_corpus[i+1], lower_case_corpus[i+2])\n",
        "\n",
        "    # Keeping track of the bigram counts\n",
        "    if bigram in bigram_counts.keys():\n",
        "        bigram_counts[bigram] += 1\n",
        "    else:\n",
        "        bigram_counts[bigram] = 1\n",
        "\n",
        "    # Keeping track of trigram counts\n",
        "    if trigram in trigram_counts.keys():\n",
        "        trigram_counts[trigram] += 1\n",
        "    else:\n",
        "        trigram_counts[trigram] = 1\n",
        "\n",
        "print(\"Example, count for bigram ('the', 'king') is: \" + str(bigram_counts[('the', 'king')]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eG-ubMr9Pruc"
      },
      "outputs": [],
      "source": [
        "# Function takes sentence as input and suggests possible words that comes after the sentence\n",
        "def suggest_next_word(input_, bigram_counts, trigram_counts, vocab):\n",
        "    # Consider the last bigram of sentence\n",
        "    tokenized_input = word_tokenize(input_.lower())\n",
        "    last_bigram = tokenized_input[-2:]\n",
        "\n",
        "    # Calculating probability for each word in vocab\n",
        "    vocab_probabilities = {}\n",
        "    for vocab_word in vocab:\n",
        "        test_trigram = (last_bigram[0], last_bigram[1], vocab_word)\n",
        "        test_bigram = (last_bigram[0], last_bigram[1])\n",
        "\n",
        "        test_trigram_count = trigram_counts.get(test_trigram, 0)\n",
        "        test_bigram_count = bigram_counts.get(test_bigram, 0)\n",
        "\n",
        "        probability = test_trigram_count / test_bigram_count\n",
        "        vocab_probabilities[vocab_word] = probability\n",
        "\n",
        "    # Sorting the vocab probability in descending order to get top probable words\n",
        "    top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "    return top_suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3diOqAuUPruc",
        "outputId": "eb439ca0-e324-4c9d-ff79-6d7bd51d0646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('james', 0.17647058823529413),\n",
              " ('of', 0.1568627450980392),\n",
              " ('arthur', 0.11764705882352941)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "suggest_next_word('I am the king', bigram_counts, trigram_counts, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXKB-h0PPruc",
        "outputId": "3fbbd0c9-8564-4ffb-b262-7db622cf77ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('france', 0.3333333333333333),\n",
              " ('hearts', 0.16666666666666666),\n",
              " ('kings', 0.08333333333333333)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "suggest_next_word('I am the king of', bigram_counts, trigram_counts, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u63T6MqPrud",
        "outputId": "807ef410-38e9-4340-c4ba-de1775d91f9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 0.26666666666666666), ('.', 0.26666666666666666), (',', 0.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "suggest_next_word('I am the king of france', bigram_counts, trigram_counts, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MEH-_8tPrud",
        "outputId": "d8cb75a3-43cf-49f9-f653-48e84ed05a8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 0.2), ('germany', 0.13333333333333333), ('some', 0.06666666666666667)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "suggest_next_word('I am the king of france and', bigram_counts, trigram_counts, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQcPa0m_Prue"
      },
      "source": [
        "## Possible Tweaks\n",
        "\n",
        "* Adding corpus (nltk, scrapping, etc)\n",
        "* DIfferent model better than trigram model\n",
        "* Handling 0 counts in model (Smoothing)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}